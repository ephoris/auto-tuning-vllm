# Throwaway configuration for RedHatAI/Qwen3-30B-A3B-FP8-dynamic
study:
  name: "qwen3_30b_throwaway_test"
  
optimization:
  preset: "high_throughput" 
  # Simple single trial for throwaway testing
  # approach: "single_objective"
  # objective:
  #   metric: "output_tokens_per_second"
  #   direction: "maximize"
  #   percentile: "median"
  sampler: "botorch"  # Simple random for single trial
  n_trials: 1  # Just one trial for throwaway run

benchmark:
  benchmark_type: "guidellm"
  model: "RedHatAI/Qwen3-30B-A3B-FP8-dynamic"
  max_seconds: 300  # 5 minutes as requested
  dataset: null  # Use synthetic data
  
  # Dataset profile: 10,000 prompt tokens / 1,000 output tokens
  prompt_tokens: 10000
  output_tokens: 1000
  
  rate: 7  # Conservative rate for large model

logging:
  file_path: "/tmp/qwen3-30b-throwaway-logs"
  log_level: "INFO"

# vLLM server parameters
parameters:
  
    

  kv_cache_dtype:
    enabled: true
    options: ["fp8"]  # Fixed for FP8 model
    
  # Tunable parameters from study results
  max_num_batched_tokens:
    enabled: true
    min: 4096
    max: 16384
    step: 2048
    # Optimal value from study: 8192
    
  block_size:
    enabled: true
    min: 8
    max: 32
    step: 8
    # Optimal value from study: 16
    
  gpu_memory_utilization:
    enabled: true
    min: 0.85
    max: 0.98
    step: 0.01
    # Optimal value from study: 0.93
    
  cuda_graph_sizes:
    enabled: true
    options: [1000, 2000, 3000, 4000, 5000, 6000, 8000]
    # Optimal value from study: 5000
    
  long_prefill_token_threshold:
    enabled: true
    min: 128
    max: 512
    step: 64
    # Optimal value from study: 256
    
  enable_cuda_graphs:
    enabled: true
    options: [true, false]  # Let optimization decide
    
