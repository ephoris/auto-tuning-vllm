# Example study configuration with auto-generated study name
study:
  # name: "local_single_trial_test_3"  # <- Comment out to use auto-generation
  # Will auto-generate unique name like: study_123456
  # No database - will use local SQLite storage in temp folder
  
optimization:
  # High-throughput optimization: Maximize token generation rate
  # approach: "single_objective"
  # objective:
  #   metric: "output_tokens_per_second"  # Throughput optimization
  #   direction: "maximize"
  #   percentile: "mean"  # Use median for stable results
  # # sampler: "tpe"
  # n_trials: 1
  
  # Alternative: Use preset for common configurations
  preset: "high_throughput" 
  # options are: high_throughput, low_latency, balanced

  # Alternative: Multi-objective (throughput vs latency)
  # approach: "multi_objective"
  # objectives:
  #   - metric: "output_tokens_per_second"
  #     direction: "maximize"
  #     percentile: "mean"
  #   - metric: "request_latency"
  #     direction: "minimize"
  #     percentile: "median"
  sampler: "botorch"
  n_trials: 200
  
benchmark:
  benchmark_type: "guidellm"
  model: "RedHatAI/Qwen3-1.7B-FP8-dynamic"  
  max_seconds: 240                 
  dataset: null  # Use synthetic data
  prompt_tokens: 2000   
  output_tokens: 2000
  rate: 30             
  
logging:
  file_path: "/tmp/auto-tune-vllm-local-run/logs"  # Local temp folder
  log_level: "INFO"


  
parameters:
  max_num_batched_tokens:
    enabled: true
    # Uses schema defaults
    
  gpu_memory_utilization:
    enabled: true
    min: 0.9
    max: 0.95
    step: 0.01
    
  kv_cache_dtype:
    enabled: true
    options: ["auto", "fp8"]
    
  enable_cuda_graphs:
    enabled: false
